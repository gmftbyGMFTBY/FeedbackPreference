# FeedbackPreference

Tian Lan, Ziao Ma, RongCheng Tu, Chen Xu, Heyan Huang, Xian-ling Mao
Beijing Insititute of Technology

#### TL;DR: We introduce a corpus for critique-tuning, which includes pairs of feedbacks for preference learning, like PPO or DPO (RLHF), aiming to improve the alignment between generated feedback with human judgments.

## Introduction

## Dataset Link

## Road Map

- [ ] Annotate High-quality Test Set
- [ ] Training Llama2-7B Model with SFT and DPO

## Citation 

Please cite our paper if Shepherd contributes in your work:

```bibtex 
@misc{Tian_Feedback_Preference_2023,
author = {Tian, Lan},
month = dec,
title = {{Feedback Preference}},
url = {https://github.com/gmftbyGMFTBY/FeedbackPreference},
year = {2023}
}
```
